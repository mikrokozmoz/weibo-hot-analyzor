import pandas as pd
from openai import OpenAI
import time
import os
from . import settings  # å¯¼å…¥é…ç½®æ–‡ä»¶ / Import configuration file

# --- åˆå§‹åŒ–å®¢æˆ·ç«¯ ---
# --- Initialize Client ---
client = OpenAI(
    api_key=settings.API_KEY, 
    base_url=settings.BASE_URL
)

# --- è¾…åŠ©å‡½æ•° ---
# --- Utility Functions ---

def load_prompt(filename):
    """è¯»å– Prompt æ¨¡æ¿æ–‡ä»¶"""
    """Load Prompt template file"""
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            return f.read()
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ {filename}ï¼Œè¯·æ£€æŸ¥è·¯å¾„ã€‚")
        print(f"âŒ Error: File not found {filename}, please check the path.")
        return ""

def get_completion(messages, temperature):
    """å°è£… API è°ƒç”¨ï¼Œå¸¦é‡è¯•æœºåˆ¶"""
    """Wrap API call with retry mechanism"""
    for i in range(settings.MAX_RETRIES):
        try:
            completion = client.chat.completions.create(
                model=settings.MODEL_NAME,
                messages=messages,
                temperature=temperature,
            )
            return completion.choices[0].message.content
        except Exception as e:
            print(f"âš ï¸ APIè°ƒç”¨æ³¢åŠ¨ (ç¬¬{i+1}/{settings.MAX_RETRIES}æ¬¡): {e}")
            print(f"âš ï¸ API call fluctuation (Attempt {i+1}/{settings.MAX_RETRIES}): {e}")
            time.sleep(2)
    return None

# --- ä¸»é€»è¾‘ ---
# --- Main Logic ---

def main():
    print(f"ğŸš€ ä»»åŠ¡å¯åŠ¨... ä½¿ç”¨æ¨¡å‹: {settings.MODEL_NAME}")
    print(f"ğŸš€ Task started... Using model: {settings.MODEL_NAME}")

    # A. åŠ è½½ Prompt æ¨¡æ¿
    # A. Load Prompt templates
    sys_prompt = load_prompt(settings.PROMPT_SYSTEM_FILE)
    stage1_prompt = load_prompt(settings.PROMPT_STAGE1_FILE)
    stage2_prompt = load_prompt(settings.PROMPT_STAGE2_FILE)

    if not (sys_prompt and stage1_prompt and stage2_prompt):
        print("âŒ ç¼ºå°‘ Prompt æ¨¡æ¿æ–‡ä»¶ï¼Œç¨‹åºç»ˆæ­¢ã€‚")
        print("âŒ Missing Prompt template files, program terminated.")
        return 

    # B. è¯»å–æ•°æ®
    # B. Read data
    try:
        df = pd.read_csv(settings.INPUT_FILE)
        # å…¼å®¹æ€§å¤„ç†ï¼šé˜²æ­¢åˆ—åä¸å¯¹
        # Compatibility handling: prevent incorrect column names
        if 'å¾®åšæ­£æ–‡' not in df.columns:
            # å‡è®¾ç¬¬2åˆ—æ˜¯æ­£æ–‡ï¼Œè‡ªåŠ¨é‡å‘½å
            # Assume 2nd column is content, auto rename
            df.rename(columns={df.columns[1]: 'å¾®åšæ­£æ–‡'}, inplace=True)
            print("âš ï¸ å·²è‡ªåŠ¨å°†ç¬¬äºŒåˆ—é‡å‘½åä¸º 'å¾®åšæ­£æ–‡'")
            print("âš ï¸ Automatically renamed second column to 'å¾®åšæ­£æ–‡'")
    except Exception as e:
        print(f"âŒ è¯»å–æ•°æ®æ–‡ä»¶å¤±è´¥: {e}")
        print(f"âŒ Failed to read data file: {e}")
        return

    # C. é¢„å¤„ç†ï¼šæŒ‰å…³é”®è¯åˆå¹¶æ–‡æœ¬
    # C. Preprocessing: Merge text by keyword
    separator = "\n\nã€---ä¸‹ä¸€æ¡å¾®åš---ã€‘\n\n"
    # ç¡®ä¿å†…å®¹è½¬ä¸ºå­—ç¬¦ä¸²å¹¶åˆå¹¶
    # Ensure content is converted to string and merged
    grouped = df.groupby('å…³é”®è¯')['å¾®åšæ­£æ–‡'].apply(lambda x: separator.join(x.astype(str))).reset_index()
    
    stage1_results_dict = {}
    
    print(f"ğŸ“Š å…±å‘ç° {len(grouped)} ä¸ªå…³é”®è¯ã€‚")
    print(f"ğŸ“Š Found {len(grouped)} keywords in total.")

    # ==========================
    # Phase 1: å¾®è§‚åˆ†æ (Map)
    # Phase 1: Micro-analysis (Map)
    # ==========================
    print("\n--- Step 1: å¾®è§‚äº‹å®æå– ---")
    print("\n--- Step 1: Micro-fact Extraction ---")
    
    for index, row in grouped.iterrows():
        kw = row['å…³é”®è¯']
        raw_text = row['å¾®åšæ­£æ–‡']
        
        # é•¿åº¦æˆªæ–­
        # Length truncation
        if settings.MAX_TEXT_LENGTH > 0 and len(raw_text) > settings.MAX_TEXT_LENGTH:
            raw_text = raw_text[:settings.MAX_TEXT_LENGTH] + "\n...(å†…å®¹è¿‡é•¿å·²æˆªæ–­)... / ...(content truncated due to length)..."
            
        print(f"   -> æ­£åœ¨åˆ†æ [{kw}] (é•¿åº¦: {len(raw_text)})")
        print(f"   -> Analyzing [{kw}] (length: {len(raw_text)})")
        
        messages = [
            {'role': 'system', 'content': sys_prompt},
            {'role': 'user', 'content': stage1_prompt.format(keyword=kw, content=raw_text)}
        ]
        
        result = get_completion(messages, temperature=settings.TEMP_STAGE_1)
        
        if result:
            stage1_results_dict[kw] = result
        else:
            stage1_results_dict[kw] = "ï¼ˆåˆ†æå¤±è´¥ï¼‰/ (Analysis failed)"

    # ä¿å­˜é˜¶æ®µä¸€ç»“æœ
    # Save Stage 1 results
    pd.DataFrame(list(stage1_results_dict.items()), columns=['å…³é”®è¯', 'å¾®è§‚åˆ†æç»“æœ'])\
      .to_csv(settings.OUTPUT_STAGE1_CSV, index=False, encoding='utf-8-sig')
    print(f"âœ… é˜¶æ®µä¸€å®Œæˆï¼Œå·²ä¿å­˜è‡³ {settings.OUTPUT_STAGE1_CSV}")
    print(f"âœ… Stage 1 completed, saved to {settings.OUTPUT_STAGE1_CSV}")

    # ==========================
    # Phase 2: å®è§‚åˆ†æ (Reduce)
    # Phase 2: Macro-analysis (Reduce)
    # ==========================
    print("\n--- Step 2: å…¨å±€å…³è”åˆ†æ ---")
    print("\n--- Step 2: Global Correlation Analysis ---")
    
    # æ‹¼æ¥æ‰€æœ‰çš„å¾®è§‚æ‘˜è¦
    # Concatenate all micro-summaries
    combined_summaries = ""
    for kw, summary in stage1_results_dict.items():
        combined_summaries += f"=== å…³äºå…³é”®è¯ã€{kw}ã€‘çš„äº‹å®æ‘˜è¦ === / === Fact Summary for Keyword ã€{kw}ã€‘ ===\n{summary}\n\n"
        
    messages_s2 = [
        {'role': 'system', 'content': sys_prompt},
        {'role': 'user', 'content': stage2_prompt.format(all_summaries=combined_summaries)}
    ]
    
    final_analysis = get_completion(messages_s2, temperature=settings.TEMP_STAGE_2)
    
    if final_analysis:
        # ä¿å­˜åˆ†ææŠ¥å‘Š
        # Save analysis report
        with open(settings.OUTPUT_STAGE2_MD, "w", encoding="utf-8") as f:
            f.write(final_analysis)
        print(f"âœ… é˜¶æ®µäºŒå®Œæˆï¼Œå·²ä¿å­˜è‡³ {settings.OUTPUT_STAGE2_MD}")
        print(f"âœ… Stage 2 completed, saved to {settings.OUTPUT_STAGE2_MD}")
        
        # ==========================
        # Phase 3: ç”ŸæˆçŸ¥è¯†åº“æ–‡ä»¶
        # Phase 3: Generate Knowledge Base File
        # ==========================
        with open(settings.OUTPUT_FINAL_CONTEXT, "w", encoding="utf-8") as f:
            f.write("ã€è‡ªåŠ¨ç”Ÿæˆçš„èƒŒæ™¯çŸ¥è¯†åº“ã€‘ / ã€Auto-generated Background Knowledge Baseã€‘\n\n")
            f.write("#####################################################\n")
            f.write("PART 1: å…¨å±€äº‹ä»¶èƒŒæ™¯ä¸å…³è”ç½‘ç»œ (Global Context)\n")
            f.write("PART 1: Global Event Background and Correlation Network\n")
            f.write("#####################################################\n\n")
            f.write(final_analysis)
            f.write("\n\n")
            f.write("#####################################################\n")
            f.write("PART 2: å…³é”®è¯å¾®è§‚äº‹å®åº“ (Fact Dictionary)\n")
            f.write("PART 2: Keyword Micro-fact Dictionary\n")
            f.write("#####################################################\n\n")
            f.write(combined_summaries)
            
        print(f"ğŸ‰ æœ€ç»ˆçŸ¥è¯†åº“å·²ç”Ÿæˆ: {settings.OUTPUT_FINAL_CONTEXT}")
        print(f"ğŸ‰ Final knowledge base generated: {settings.OUTPUT_FINAL_CONTEXT}")
        print("ğŸ’¡ æç¤º: ä¸‹ä¸€æ­¥æ‰“æ ‡æ—¶ï¼Œè¯·ç›´æ¥è¯»å–è¿™ä¸ªæ–‡ä»¶çš„å†…å®¹ä½œä¸º Contextã€‚")
        print("ğŸ’¡ Tip: For the next labeling step, please read this file's content as Context.")
    else:
        print("âŒ é˜¶æ®µäºŒåˆ†æå¤±è´¥ï¼Œæ— æ³•ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šã€‚")
        print("âŒ Stage 2 analysis failed, unable to generate final report.")

if __name__ == "__main__":
    main()