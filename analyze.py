#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ•°æ®åˆ†æä¸»ç¨‹åºï¼šè‡ªåŠ¨åˆå§‹åŒ–processing submoduleï¼ŒåŠ è½½åˆå¹¶æ•°æ®ï¼Œè¿›è¡Œåˆ†æ
Data analysis main program: Auto initialize processing submodule, load merged data, perform analysis
"""

import subprocess
import os
import sys
import argparse
import pandas as pd

def init_processing_submodule():
    """è‡ªåŠ¨åˆå§‹åŒ–processing submoduleï¼ˆå¦‚æœè¿˜æœªåˆå§‹åŒ–ï¼‰"""
    processing_path = 'processing'
    
    if not os.path.exists(os.path.join(processing_path, '.git')):
        print("ğŸ“¥ åˆå§‹åŒ–æ•°æ®å¤„ç†æ¨¡å—...")
        print("ğŸ“¥ Initializing data processing module...")
        try:
            subprocess.run(['git', 'submodule', 'update', '--init', '--recursive'], check=True)
            print("âœ… æ•°æ®å¤„ç†æ¨¡å—åˆå§‹åŒ–æˆåŠŸ")
            print("âœ… Data processing module initialized successfully")
        except Exception as e:
            print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
            print(f"âŒ Initialization failed: {e}")
            sys.exit(1)
    else:
        print("âœ… æ•°æ®å¤„ç†æ¨¡å—å·²å°±ç»ª")
        print("âœ… Data processing module is ready")

def load_data():
    """åŠ è½½åˆå¹¶åçš„åŸå§‹æ•°æ®"""
    data_path = 'files/data_raw.csv'
    
    if not os.path.exists(data_path):
        print(f"âŒ æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶: {data_path}")
        print(f"âŒ Data file not found: {data_path}")
        print("âš ï¸  è¯·å…ˆè¿è¡Œ merge_results.py æ¥åˆå¹¶çˆ¬è™«ç»“æœ")
        print("âš ï¸  Please run merge_results.py first to merge crawler results")
        sys.exit(1)
    
    try:
        import pandas as pd
        df = pd.read_csv(data_path, encoding='utf-8')
        print(f"âœ… å·²åŠ è½½æ•°æ®: {data_path}")
        print(f"âœ… Data loaded: {data_path}")
        print(f"ğŸ“Š æ•°æ®è§„æ¨¡: {len(df)} è¡Œ, {len(df.columns)} åˆ—")
        print(f"ğŸ“Š Data shape: {len(df)} rows, {len(df.columns)} columns")
        return df
    except Exception as e:
        print(f"âŒ åŠ è½½æ•°æ®å¤±è´¥: {e}")
        print(f"âŒ Failed to load data: {e}")
        sys.exit(1)

def deduplicate_data(df):
    """å»é‡æ•°æ®"""
    try:
        from processing.post_analysis import pre_processing
        
        print("\n" + "=" * 50)
        print("å¼€å§‹å»é‡æ•°æ®...")
        print("Starting data deduplication...")
        print("=" * 50)
        
        # è·å–ç›¸ä¼¼åº¦é˜ˆå€¼
        print("\nğŸšï¸  è®¾ç½®ç›¸ä¼¼åº¦é˜ˆå€¼ / Set similarity threshold")
        print("=" * 50)
        print("ç›¸ä¼¼åº¦é˜ˆå€¼è¯´æ˜:")
        print("Similarity threshold explanation:")
        print("  - é˜ˆå€¼è¶Šé«˜ï¼Œå»é‡è¶Šä¸¥æ ¼ï¼ˆåªæœ‰æå…¶ç›¸ä¼¼çš„æ‰ä¼šè¢«å»é‡ï¼‰")
        print("    Higher threshold = stricter dedup (only very similar posts removed)")
        print("  - é˜ˆå€¼è¶Šä½ï¼Œå»é‡è¶Šå®½æ¾ï¼ˆç›¸ä¼¼ç¨‹åº¦è¾ƒä½çš„ä¹Ÿä¼šè¢«å»é‡ï¼‰")
        print("    Lower threshold = looser dedup (more posts removed)")
        print("  - æ¨èèŒƒå›´: 0.75 - 0.95")
        print("    Recommended range: 0.75 - 0.95")
        print("  - é»˜è®¤å€¼: 0.88")
        print("    Default value: 0.88")
        print("\nğŸ’¡ ä½¿ç”¨é»˜è®¤å€¼: ç›´æ¥æŒ‰å›è½¦é”® (Enter)")
        print("ğŸ’¡ Use default value: Just press Enter")
        print("=" * 50)
        
        while True:
            try:
                threshold_input = input("\nè¯·è¾“å…¥ç›¸ä¼¼åº¦é˜ˆå€¼ (0-1) / Enter similarity threshold (0-1) [é»˜è®¤ 0.88]: ").strip()
                
                # å¦‚æœä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤å€¼
                if not threshold_input:
                    similarity_threshold = 0.88
                    print(f"âœ… ä½¿ç”¨é»˜è®¤å€¼: {similarity_threshold}")
                    print(f"âœ… Using default value: {similarity_threshold}")
                    break
                
                # å°è¯•è½¬æ¢ä¸ºæµ®ç‚¹æ•°
                similarity_threshold = float(threshold_input)
                
                # æ£€æŸ¥æ˜¯å¦åœ¨0åˆ°1ä¹‹é—´
                if 0 <= similarity_threshold <= 1:
                    print(f"âœ… å·²è®¾ç½®é˜ˆå€¼: {similarity_threshold}")
                    print(f"âœ… Threshold set: {similarity_threshold}")
                    break
                else:
                    print("âŒ è¾“å…¥é”™è¯¯ï¼Œè¯·è¾“å…¥0åˆ°1ä¹‹é—´çš„æ•°å€¼")
                    print("âŒ Invalid input, please enter a value between 0 and 1")
            except ValueError:
                print("âŒ è¾“å…¥é”™è¯¯ï¼Œè¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å€¼")
                print("âŒ Invalid input, please enter a valid number")
        
        # è°ƒç”¨dedupe_postså‡½æ•°
        df_deduped = pre_processing.dedupe_posts(df, similarity_threshold=similarity_threshold)
        
        # ä¿å­˜å»é‡åçš„æ•°æ®
        output_dir = 'files'
        os.makedirs(output_dir, exist_ok=True)  # ç¡®ä¿filesç›®å½•å­˜åœ¨
        output_path = os.path.join(output_dir, 'data_deduped.csv')
        df_deduped.to_csv(output_path, index=False, encoding='utf-8')
        
        print(f"\nâœ… å»é‡å®Œæˆ!")
        print(f"âœ… Deduplication completed!")
        print(f"ğŸ“Š åŸå§‹æ•°æ®: {len(df)} è¡Œ")
        print(f"ğŸ“Š Original data: {len(df)} rows")
        print(f"ğŸ“Š å»é‡å: {len(df_deduped)} è¡Œ (ç§»é™¤ {len(df) - len(df_deduped)} æ¡é‡å¤)")
        print(f"ğŸ“Š After dedup: {len(df_deduped)} rows (removed {len(df) - len(df_deduped)} duplicates)")
        print(f"ğŸ“ å·²ä¿å­˜åˆ°: {output_path}")
        print(f"ğŸ“ Saved to: {output_path}")
        
        return df_deduped
    except Exception as e:
        print(f"âŒ å»é‡å¤±è´¥: {e}")
        print(f"âŒ Deduplication failed: {e}")
        sys.exit(1)

def analyze_data(df):
    """
    ä½¿ç”¨processingæ¨¡å—è¿›è¡Œæ•°æ®åˆ†æ
    
    ç›®å‰æ”¯æŒçš„åŠŸèƒ½ï¼š
    - æ–‡æœ¬é¢„å¤„ç†ï¼ˆæ¸…æ´—ã€åˆ†è¯ç­‰ï¼‰
    - è¯­æ–™åˆ†æï¼ˆè¯é¢‘ç»Ÿè®¡ã€è¯äº‘ç­‰ï¼‰
    """
    try:
        # å¯¼å…¥processingæ¨¡å—
        from processing.post_analysis import pre_processing, corpus_analysis
        
        print("\n" + "=" * 50)
        print("å¼€å§‹æ•°æ®åˆ†æ...")
        print("Starting data analysis...")
        print("=" * 50)
        
        # è¿™é‡Œå¯ä»¥æ ¹æ®éœ€è¦è°ƒç”¨processingæ¨¡å—ä¸­çš„å„ä¸ªå‡½æ•°
        # ä¾‹å¦‚ï¼š
        # df['cleaned_text'] = df['å†…å®¹'].apply(corpus_analysis.clean_text)
        # freq_stats = corpus_analysis.word_frequency_analysis(df['cleaned_text'])
        
        print("\nğŸ’¡ å¯ç”¨çš„å¤„ç†å‡½æ•°ï¼š")
        print("ğŸ’¡ Available processing functions:")
        print("  - corpus_analysis.clean_text() - æ–‡æœ¬æ¸…æ´—")
        print("  - corpus_analysis.word_segmentation() - åˆ†è¯")
        print("  - corpus_analysis.word_frequency_analysis() - è¯é¢‘åˆ†æ")
        print("  - pre_processing.load_posts_from_folder() - åŠ è½½æ–‡ä»¶å¤¹ä¸­çš„CSVæ–‡ä»¶")
        
        return df
    except ImportError as e:
        print(f"âŒ å¯¼å…¥processingæ¨¡å—å¤±è´¥: {e}")
        print(f"âŒ Failed to import processing module: {e}")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ æ•°æ®åˆ†æå¤±è´¥: {e}")
        print(f"âŒ Data analysis failed: {e}")
        sys.exit(1)

if __name__ == "__main__":
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(
        description='å¾®åšæ•°æ®åˆ†æå·¥å…· / Weibo Data Analysis Tool',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ / Examples:
  python analyze.py --dedup         # å»é‡æ•°æ®
  python analyze.py --help          # æŸ¥çœ‹å¸®åŠ©
        """
    )
    
    parser.add_argument('--dedup', action='store_true', 
                       help='å»é‡åŸå§‹æ•°æ®å¹¶ä¿å­˜ä¸º data_deduped.csv')
    
    args = parser.parse_args()
    
    print("=" * 50)
    print("å¾®åšæ•°æ®åˆ†æ - è‡ªåŠ¨å¯åŠ¨ç¨‹åº")
    print("Weibo Data Analysis - Auto Launcher")
    print("=" * 50 + "\n")
    
    # åˆå§‹åŒ–submodule
    init_processing_submodule()
    
    # æ ¹æ®å‘½ä»¤è¡Œå‚æ•°æ‰§è¡Œä¸åŒçš„æ“ä½œ
    if args.dedup:
        print()
        df = load_data()
        print()
        deduplicate_data(df)
    else:
        print("\nâš ï¸  è¯·æŒ‡å®šæ“ä½œå‚æ•°")
        print("âš ï¸  Please specify an operation parameter")
        print("\nå¯ç”¨çš„æ“ä½œ / Available operations:")
        print("  --dedup          å»é‡æ•°æ® (Deduplicate data)")
        print("\nä½¿ç”¨ --help æŸ¥çœ‹æ›´å¤šå¸®åŠ© / Use --help for more information")
