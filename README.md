# å¾®åšçƒ­æœåˆ†æå·¥å…· | *Weibo Hot Search Analyzer*

ä¸€ä¸ªç”¨äºå¾®åšæ•°æ®åˆ†æå’Œå¤„ç†çš„å®Œæ•´å·¥å…·é“¾ã€‚æ”¯æŒæ™ºèƒ½å»é‡ã€åˆ†è¯ç»Ÿè®¡ã€è¯äº‘ç”Ÿæˆã€ä»¥åŠAIé©±åŠ¨çš„æ‘˜è¦åˆ†æã€‚ï¼ˆå¯é€‰ï¼šæ”¯æŒæ•°æ®åˆå¹¶åŠŸèƒ½ï¼‰

*A complete toolkit for Weibo data analysis and processing. Supports smart deduplication, word frequency statistics, word cloud generation, and AI-driven summary analysis. (Optional: supports data merging functionality)*

---

## é¡¹ç›®ç®€ä»‹ | *Project Overview*

æœ¬é¡¹ç›®ä¸“æ³¨äºå¾®åšæ•°æ®çš„åˆ†æå’Œå¤„ç†ï¼Œé€šè¿‡æ¨¡å—åŒ–çš„è®¾è®¡ï¼Œç”¨æˆ·å¯ä»¥çµæ´»åœ°ï¼š
- ä½¿ç”¨ `utils` æ¨¡å—è¿›è¡Œæ•°æ®é¢„å¤„ç†ï¼ˆå»é‡ã€åˆ†è¯ã€è¯äº‘ç”Ÿæˆç­‰ï¼‰
- ä½¿ç”¨ `analyzer` æ¨¡å—è¿›è¡Œ AI é©±åŠ¨çš„åˆ†æå’Œæ‘˜è¦ç”Ÿæˆ
- è‡ªå®šä¹‰å‚æ•°ï¼Œä¸€é”®æ‰§è¡Œå®Œæ•´çš„æ•°æ®å¤„ç†æµç¨‹
- ï¼ˆå¯é€‰ï¼‰ä½¿ç”¨æ•°æ®åˆå¹¶åŠŸèƒ½æ•´åˆå¤šä¸ªæ¥æºçš„æ•°æ®

*This project focuses on analysis and processing of Weibo data. With a modular design, users can flexibly:*
- *Use the `utils` module for data preprocessing (deduplication, tokenization, word cloud generation, etc.)*
- *Use the `analyzer` module for AI-driven analysis and summary generation*
- *Customize parameters and execute the complete data processing pipeline with a single command*
- *(Optional) Use data merging functionality to consolidate data from multiple sources*

---

## åŠŸèƒ½ç‰¹æ€§ | *Features*

### æ•°æ®å¤„ç†æ¨¡å— | *Data Processing Module*

- **çµæ´»çš„æ•°æ®åŠ è½½** ğŸ“‚ï¼šä»æ–‡ä»¶å¤¹ç›´æ¥åŠ è½½ CSV æ–‡ä»¶
- **æ™ºèƒ½å»é‡** âœ¨ï¼šæ”¯æŒè‡ªå®šä¹‰ç›¸ä¼¼åº¦é˜ˆå€¼çš„å»é‡ï¼Œä¿ç•™æœ€æ—©å‘å¸ƒçš„è®°å½•
- **è¯é¢˜æå–** ğŸ·ï¸ï¼šè‡ªåŠ¨ä»å¾®åšä¸­æå–å‰ä¸‰ä¸ªè¯é¢˜
- **åˆ†è¯ç»Ÿè®¡** ğŸ”¤ï¼šåŸºäº jieba çš„ä¸­æ–‡åˆ†è¯å’Œè¯é¢‘ç»Ÿè®¡
- **è¯äº‘ç”Ÿæˆ** â˜ï¸ï¼šæŒ‰å…³é”®è¯è‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡è¯äº‘å›¾
- **å‚æ•°é›†ä¸­ç®¡ç†** âš™ï¸ï¼šæ‰€æœ‰å¤„ç†å‚æ•°åœ¨ `utils/settings.py` ä¸­é…ç½®

<br>

- *Flexible data loading from CSV files in a folder* 
- *Smart deduplication with custom similarity threshold* 
- *Automatic topic extraction from posts* 
- *Chinese word segmentation and frequency statistics based on jieba*
- *Auto-generate high-quality word clouds per keyword*
- *Centralized parameter management in `utils/settings.py`*

### AI åˆ†ææ¨¡å— | *AI Analysis Module*

- **ä¸¤é˜¶æ®µåˆ†æ** ğŸ§ ï¼šç¬¬ä¸€é˜¶æ®µå¾®è§‚äº‹å®æå–ï¼ˆé€å…³é”®è¯åˆ†æï¼‰+ ç¬¬äºŒé˜¶æ®µå®è§‚å…³è”åˆ†æï¼ˆå…¨å±€äº‹ä»¶å…³è”ï¼‰
- **çŸ¥è¯†åº“ç”Ÿæˆ** ğŸ“šï¼šè‡ªåŠ¨ç”ŸæˆèƒŒæ™¯çŸ¥è¯†åº“ï¼Œæ”¯æŒä¸‹æ¸¸åº”ç”¨
- **æ¨¡å‹çµæ´»é…ç½®** ğŸ”§ï¼šæ”¯æŒå¤šç§é˜¿é‡Œäº‘ç™¾ç‚¼å¤§æ¨¡å‹ï¼ˆqwen-plusã€qwen-max ç­‰ï¼‰
- **å®Œå–„çš„é”™è¯¯å¤„ç†** ğŸ›¡ï¸ï¼šAPI è°ƒç”¨å¤±è´¥è‡ªåŠ¨é‡è¯•æœºåˆ¶

<br>

- *Two-stage analysis (micro-fact extraction + macro-correlation analysis)*
- *Automatic knowledge base generation for downstream applications*
- *Flexible model configuration supporting multiple Alibaba Cloud models*
- *Robust error handling with automatic retry on API failures*

---

## å¿«é€Ÿä¸Šæ‰‹ | *Quick Start*

### 1. ç¯å¢ƒå®‰è£… | *Installation*

```bash
# å…‹éš†é¡¹ç›®
# Clone the repository
git clone --recursive https://github.com/mikrokozmoz/weibo-hot-analyzer
cd weibo-hot-analyzer

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼ˆå¯é€‰ä½†æ¨èï¼‰
# Create virtual environment (optional but recommended)
python -m venv venv
source venv/bin/activate  # on Windows: venv\Scripts\activate

# å®‰è£…ä¾èµ–
# Install dependencies
pip install -r requirements.txt
```

### 2. æ•°æ®é¢„å¤„ç† | *Data Preprocessing*

ä½¿ç”¨ `utils` æ¨¡å—å¤„ç† CSV æ•°æ®ï¼ˆå»é‡ã€åˆ†è¯ã€è¯äº‘ç­‰ï¼‰ã€‚

*Use the `utils` module to process CSV data (deduplication, tokenization, word clouds, etc.).*

#### 2.1 é…ç½®å¤„ç†å‚æ•° | *Configure Processing Parameters*

ç¼–è¾‘ `utils/settings.py`ï¼Œè®¾ç½®ï¼š
- `LOAD_POSTS_FOLDER_PATH`ï¼šåŒ…å« CSV æ–‡ä»¶çš„æ–‡ä»¶å¤¹è·¯å¾„
- å…¶ä»–å¤„ç†å‚æ•°ï¼ˆå»é‡é˜ˆå€¼ã€è¯é•¿åº¦èŒƒå›´ç­‰ï¼‰

*Edit `utils/settings.py` to set:*
- *`LOAD_POSTS_FOLDER_PATH`: Path to folder containing CSV files*
- *Other processing parameters (deduplication threshold, word length range, etc.)*

#### 2.2 æ‰§è¡Œæ•°æ®å¤„ç† | *Execute Data Processing*

```bash
# æ–¹å¼ä¸€ï¼šä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°
# Method 1: Using command-line arguments
python -m utils --load_files_from_folder --dedupe

# æ–¹å¼äºŒï¼šåœ¨ Python ä»£ç ä¸­è°ƒç”¨
# Method 2: Call in Python code
from utils import data_processing

result = data_processing(
    load_files_from_folder=True,
    extract_topics=True,
    dedupe=True,
    tokenize=True,
    word_frequency=True,
    create_wordcloud=True
)

# è®¿é—®ç»“æœ / Access results
df = result['df']  # å¤„ç†åçš„ DataFrame
word_freq = result['word_freq_by_keyword']  # è¯é¢‘å­—å…¸
```

**æ”¯æŒçš„å‚æ•°** | *Supported parameters*:
- `--load_files_from_folder`ï¼šä»æ–‡ä»¶å¤¹åŠ è½½ CSV / Load CSV from folder
- `--extract_topics`ï¼šæå–è¯é¢˜ / Extract topics
- `--dedupe`ï¼šå»é‡å¤„ç† / Deduplication
- `--tokenize`ï¼šåˆ†è¯ç»Ÿè®¡ / Word segmentation & frequency
- `--word_frequency`ï¼šç”Ÿæˆè¯é¢‘è¡¨ / Generate frequency table
- `--create_wordcloud`ï¼šç”Ÿæˆè¯äº‘ / Generate word clouds

### 3. AI é©±åŠ¨åˆ†æ | *AI-Driven Analysis*

ä½¿ç”¨ `analyzer` æ¨¡å—è¿›è¡Œ AI åˆ†æå’Œæ‘˜è¦ç”Ÿæˆã€‚

*Use the `analyzer` module for AI analysis and summary generation.*

#### 3.1 é…ç½® AI å‚æ•° | *Configure AI Parameters*

ç¼–è¾‘ `analyzer/settings.py`ï¼Œè®¾ç½®ï¼š
- `API_KEY`ï¼šé˜¿é‡Œäº‘ç™¾ç‚¼ API Keyï¼ˆä» https://bailian.console.aliyun.com/ è·å–ï¼‰
- `MODEL_NAME`ï¼šé€‰æ‹©æ¨¡å‹ï¼ˆæ¨è qwen-plus æˆ– qwen-maxï¼‰
- `INPUT_FILE`ï¼šå¾…åˆ†æçš„ CSV æ–‡ä»¶è·¯å¾„
- å…¶ä»–åˆ†æå‚æ•°

*Edit `analyzer/settings.py` to set:*
- *`API_KEY`: Alibaba Cloud Bailian API Key (get from https://bailian.console.aliyun.com/)*
- *`MODEL_NAME`: Choose model (recommended qwen-plus or qwen-max)*
- *`INPUT_FILE`: Path to CSV file to analyze*
- *Other analysis parameters*

#### 3.2 è¿è¡Œåˆ†æ | *Run Analysis*

```bash
# æ‰§è¡Œä¸¤é˜¶æ®µ AI åˆ†æ
python analyzer/summary.py

# è¾“å‡ºç»“æœï¼š
# Output results:
# - stage1_keyword_analysis.csv: å…³é”®è¯å¾®è§‚åˆ†æ / Micro-analysis per keyword
# - stage2_correlation_analysis_report.md: å®è§‚å…³è”æŠ¥å‘Š / Keyword-correlation report
# - final_context_knowledge_base.txt: å®Œæ•´çŸ¥è¯†åº“ / Complete knowledge base
```

#### 3.3 æµ‹è¯•æ‰“æ ‡æ•ˆæœ | *Test Labeling*

```bash
python -m analyzer.labeling_testing

# è¾“å‡ºï¼š
# - labeling_test_results.csv: æ‰“æ ‡æµ‹è¯•ç»“æœ
```

#### 3.4 æ‰¹é‡æ¨ç† | *Batch Inference*

ä½¿ç”¨é˜¿é‡Œäº‘ç™¾ç‚¼çš„æ‰¹é‡æ¨ç†æœåŠ¡å¯¹å¤§è§„æ¨¡æ•°æ®è¿›è¡Œæ ‡æ³¨ã€‚

*Use Alibaba Cloud Bailian's batch inference service for large-scale labeling.*

**æ­¥éª¤ 1: ç”Ÿæˆæ‰¹é‡æ¨ç†è¯·æ±‚ | *Step 1: Generate Batch Requests*

```bash
python -m analyzer.batch_generator

# åŠŸèƒ½è¯´æ˜ / Features:
# - è¯»å– post_list.csv
# - è‡ªåŠ¨æŒ‰ id å»é‡ï¼ˆä¿ç•™ç¬¬ä¸€æ¡è®°å½•çš„æ­£æ–‡ï¼‰
# - ç”Ÿæˆ batch_list.jsonl è¯·æ±‚æ–‡ä»¶
# - ä½¿ç”¨ labeling_prompt.txt ä½œä¸º system prompt
#
# Output: analyzer/data/batch_list.jsonl
```

**æ­¥éª¤ 2: ä¸Šä¼ è‡³ç™¾ç‚¼æ‰¹é‡æ¨ç†æœåŠ¡ | *Step 2: Upload to Alibaba Batch Service*

1. è®¿é—® https://dashscope.aliyuncs.com ï¼ˆé€‰æ‹©æ‰¹é‡æ¨ç†æœåŠ¡ï¼‰
2. ä¸Šä¼  `batch_list.jsonl` æ–‡ä»¶
3. ç­‰å¾…ä»»åŠ¡å®Œæˆï¼Œè·å–ç»“æœ URL

*Visit https://dashscope.aliyuncs.com and use batch inference service*

**æ­¥éª¤ 3: ä¸‹è½½å¹¶å¤„ç†ç»“æœ | *Step 3: Download & Process Results*

```bash
# åœ¨ analyzer/settings.py ä¸­æ›´æ–° RESULT_URL
# Update RESULT_URL in analyzer/settings.py

python -m analyzer.result_download_and_conversion

# è¾“å‡ºç»“æœï¼š
# 1. batch_results_raw.jsonl - åŸå§‹è¿”å›ç»“æœ
# 2. batch_results_final.csv - å»é‡åçš„ä¸‰åˆ—æ ¼å¼ï¼ˆidã€contentã€labelï¼‰
# 3. batch_results_final_expanded.csv - æ ‡ç­¾å±•å¼€ç‰ˆæœ¬ï¼ˆ10åˆ—ï¼šidã€contentã€validityã€stanceã€emotion_categoryã€emotion_subtypeã€targetã€mf_mainã€mf_directionã€reasoningï¼‰
```

**å…³é”®ç‰¹æ€§ | *Key Features*

- âœ… **è‡ªåŠ¨å»é‡** | Automatic deduplicationï¼šä¸ batch_generator ä¸€è‡´çš„å»é‡é€»è¾‘ï¼Œé¿å…é‡å¤æ ‡æ³¨
- âœ… **æˆæœ¬ä¼˜åŒ–** | Cost optimizationï¼šä¸åœ¨è¯·æ±‚ä½“ä¸­å­˜å‚¨åŸå§‹æ­£æ–‡ï¼Œå‡å°‘æ•°æ®ä¼ è¾“
- âœ… **å®Œæ•´è¿½æº¯** | Full traceabilityï¼šæ ¹æ® custom_id ä»åŸå§‹æ•°æ®æ¢å¤æ­£æ–‡ï¼Œä¿è¯æ•°æ®å®Œæ•´æ€§
- âœ… **æ ‡ç­¾æ‹†è§£** | Label parsingï¼šè‡ªåŠ¨å°† JSON label æ‹†è§£ä¸ºå•ç‹¬åˆ—ï¼Œæ”¯æŒåç»­åˆ†æ

---

## é¡¹ç›®ç»“æ„ | *Project Structure*

```
weibo-hot-analyzer/
â”œâ”€â”€ analyzer/                          # AI é©±åŠ¨åˆ†ææ¨¡å— / AI Analysis Module
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ settings.py                   # AI åˆ†æå‚æ•°é…ç½® / AI parameters
â”‚   â”œâ”€â”€ summary.py                    # ä¸¤é˜¶æ®µåˆ†æè„šæœ¬ / Two-stage analysis script
â”‚   â”œâ”€â”€ labeling_testing.py           # æ‰“æ ‡æµ‹è¯•å·¥å…· / Labeling testing tool
â”‚   â”œâ”€â”€ batch_generator.py            # æ‰¹é‡æ¨ç†æ–‡ä»¶ç”Ÿæˆå™¨ / Batch inference file generator (æ–° / New)
â”‚   â”œâ”€â”€ result_download_and_conversion.py  # æ‰¹é‡æ¨ç†ç»“æœå¤„ç† / Batch result processor (æ–° / New)
â”‚   â”œâ”€â”€ download_sample_results.py    # æ ·æœ¬ç»“æœä¸‹è½½å·¥å…· / Sample results downloader (æ–° / New)
â”‚   â”œâ”€â”€ data/                         # æ•°æ®ç›®å½• / Data directory
â”‚   â”‚   â”œâ”€â”€ batch_example.jsonl       # æ‰¹é‡æ¨ç†ç¤ºä¾‹æ–‡ä»¶ / Batch inference example
â”‚   â”‚   â”œâ”€â”€ batch_list.jsonl          # ç”Ÿæˆçš„æ‰¹é‡è¯·æ±‚æ–‡ä»¶ / Generated batch requests
â”‚   â”‚   â”œâ”€â”€ batch_results_raw.jsonl   # åŸå§‹æ‰¹é‡æ¨ç†ç»“æœ / Raw batch results
â”‚   â”‚   â”œâ”€â”€ batch_results_final.csv   # å¤„ç†åçš„ä¸‰åˆ—ç»“æœ / Processed three-column results
â”‚   â”‚   â””â”€â”€ batch_results_final_expanded.csv  # æ ‡ç­¾å±•å¼€ç‰ˆæœ¬ / Expanded label version
â”‚   â””â”€â”€ prompts/                      # Prompt æ¨¡æ¿æ–‡ä»¶ / Prompt templates
â”‚       â”œâ”€â”€ sys_prompt.txt            # ç³»ç»Ÿ prompt / System prompt
â”‚       â”œâ”€â”€ keyword_prompt.txt        # å…³é”®è¯åˆ†æ prompt / Keyword analysis prompt
â”‚       â”œâ”€â”€ correlation_prompt.txt    # å…³è”åˆ†æ prompt / Correlation analysis prompt
â”‚       â””â”€â”€ labeling_prompt.txt       # æ‰“æ ‡ prompt / Labeling prompt
â”‚
â”œâ”€â”€ utils/                             # æ•°æ®å¤„ç†æ¨¡å— / Data Processing Module
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ __main__.py                   # å‘½ä»¤è¡Œæ¥å£ / CLI interface
â”‚   â”œâ”€â”€ settings.py                   # å¤„ç†å‚æ•°é…ç½® / Processing parameters
â”‚   â””â”€â”€ data_processing.py            # å…¥å£å‡½æ•° / Entry function
â”‚
â”œâ”€â”€ processing/                        # Submodule: æ•°æ®å¤„ç†åº“
â”‚   â””â”€â”€ post_analysis/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ pre_processing.py         # æ•°æ®åŠ è½½ã€å»é‡ã€è¯é¢˜æå–
â”‚       â””â”€â”€ corpus_analysis.py        # åˆ†è¯ã€è¯é¢‘ã€è¯äº‘     
â”‚
â”œâ”€â”€ requirements.txt                   # é¡¹ç›®ä¾èµ– / Dependencies
â”œâ”€â”€ README.md                          # é¡¹ç›®è¯´æ˜ï¼ˆæœ¬æ–‡ä»¶ï¼‰
â””â”€â”€ .gitmodules                        # Submodule é…ç½®
```

---

## æ ¸å¿ƒæ¦‚å¿µ | *Core Concepts*

### å»é‡ç›¸ä¼¼åº¦é˜ˆå€¼ | *Deduplication Similarity Threshold*

å»é‡è¿‡ç¨‹ä¸­ï¼Œç›¸ä¼¼åº¦é˜ˆå€¼æ§åˆ¶å»é‡çš„ä¸¥æ ¼ç¨‹åº¦ï¼š

*The similarity threshold controls the strictness of deduplication:*

| é˜ˆå€¼èŒƒå›´ | è¯´æ˜ | å»ºè®®åœºæ™¯ |
|---------|------|--------|
| 0.70-0.80 | å®½æ¾å»é‡ï¼Œç§»é™¤æ›´å¤šé‡å¤ | éœ€è¦é«˜è´¨é‡å”¯ä¸€å†…å®¹çš„åˆ†æ |
| 0.80-0.90 | å¹³è¡¡å»é‡ï¼Œæ¨èä½¿ç”¨ | ä¸€èˆ¬æ•°æ®åˆ†æ |
| 0.90-0.99 | ä¸¥æ ¼å»é‡ï¼Œç§»é™¤è¾ƒå°‘é‡å¤ | ä¿ç•™ç»†å¾®å·®å¼‚å†…å®¹ |

<br>

| *Threshold* | *Description* | *Recommended Scenario* |
|---------|---------|--------|
| *0.70-0.80* | *Loose dedup, remove more* | *High-quality unique content analysis* |
| *0.80-0.90* | *Balanced dedup (recommended)* | *General data analysis* |
| *0.90-0.99* | *Strict dedup, remove fewer* | *Preserve subtle differences* |

### AI æ¨¡å‹é€‰æ‹© | *AI Model Selection*

æ”¯æŒçš„é˜¿é‡Œäº‘ç™¾ç‚¼æ¨¡å‹ï¼š

*Supported Alibaba Cloud Bailian models:*

- **qwen-plus**ï¼šæ€§ä»·æ¯”é«˜ï¼Œé€‚åˆå¤§éƒ¨åˆ†ä»»åŠ¡ / Cost-effective, suitable for most tasks
- **qwen-max**ï¼šé€»è¾‘èƒ½åŠ›å¼ºï¼Œé€‚åˆå¤æ‚åˆ†æ / Stronger logic, suitable for complex analysis
- **qwen-turbo**ï¼šé€Ÿåº¦å¿«ï¼Œé€‚åˆå¿«é€Ÿå¤„ç† / Fast speed, suitable for quick processing

---

## è¯¦ç»†æ¨¡å—è¯´æ˜ | *Module Documentation*

### 1. æ•°æ®å¤„ç†æ¨¡å— | *Data Processing Module (`utils`)*

#### æ ¸å¿ƒå‡½æ•° | *Core Function*

**`data_processing()`** - ç»Ÿä¸€çš„æ•°æ®å¤„ç†å…¥å£å‡½æ•° / Unified data processing entry function

#### å‚æ•°é…ç½® | *Settings Configuration (`utils/settings.py`)*

**æ•°æ®åŠ è½½å‚æ•° | *Load Parameters***
- `LOAD_POSTS_FOLDER_PATH` - CSV æ–‡ä»¶æ‰€åœ¨æ–‡ä»¶å¤¹è·¯å¾„ï¼ˆå¿…é¡»æŒ‡å®šï¼‰/ Folder path containing CSV files (required)
- `LOAD_POSTS_KEYWORD_COLUMN` - æ–°å¢åˆ—åç§°ï¼ˆé»˜è®¤ï¼š'å…³é”®è¯'ï¼‰/ New column name (default: 'å…³é”®è¯')

**è¯é¢˜æå–å‚æ•° | *Topic Extraction Parameters***
- `EXTRACT_TOPICS_TOPICS_COLUMN` - åŒ…å«è¯é¢˜çš„åˆ—åï¼ˆé»˜è®¤ï¼š'è¯é¢˜'ï¼‰/ Column name containing topics
- `EXTRACT_TOPICS_ID_COLUMN` - å¾®åšIDåˆ—åï¼ˆé»˜è®¤ï¼š'id'ï¼‰/ Post ID column name

**å»é‡å‚æ•° | *Deduplication Parameters***
- `DEDUPE_KEYWORD_COL` - ç”¨äºåˆ†ç»„çš„å…³é”®è¯åˆ—åï¼ˆé»˜è®¤ï¼š'å…³é”®è¯'ï¼‰/ Grouping column name
- `DEDUPE_TEXT_COL` - ç”¨äºåˆ¤æ–­é‡å¤çš„æ–‡æœ¬åˆ—ï¼ˆé»˜è®¤ï¼š'å¾®åšæ­£æ–‡_cleaned'ï¼‰/ Text column for similarity check
- `DEDUPE_TIME_COL` - ç”¨äºé€‰æ‹©ä¿ç•™è®°å½•çš„æ—¶é—´åˆ—ï¼ˆé»˜è®¤ï¼š'å‘å¸ƒæ—¶é—´'ï¼‰/ Time column for record selection
- `DEDUPE_SUM_COLS` - éœ€è¦æ±‚å’Œçš„æ•°å€¼åˆ—ï¼ˆé»˜è®¤ï¼šNoneï¼Œè‡ªåŠ¨ä¸º ['ç‚¹èµæ•°','è¯„è®ºæ•°','è½¬å‘æ•°','äº’åŠ¨æ€»æ•°']ï¼‰/ Numeric columns to sum
- `DEDUPE_SIMILARITY_THRESHOLD` - ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆé»˜è®¤ï¼š0.88ï¼‰/ Similarity threshold (0-1)
- `DEDUPE_MIN_LEN_FOR_SIMILARITY` - è®¡ç®—ç›¸ä¼¼åº¦çš„æœ€å°é•¿åº¦ï¼ˆé»˜è®¤ï¼š6ï¼‰/ Minimum length for similarity calculation
- `DEDUPE_DEBUG` - æ˜¯å¦è¾“å‡º debug ä¿¡æ¯ï¼ˆé»˜è®¤ï¼šFalseï¼‰/ Enable debug output
- `DEDUPE_AUTO_CLEAN` - è‡ªåŠ¨æ¸…æ´—æ–‡æœ¬ï¼ˆé»˜è®¤ï¼šFalseï¼‰/ Auto-clean text if column doesn't exist

**åˆ†è¯å‚æ•° | *Tokenization Parameters***
- `TOKENIZE_TEXT_COLUMN` - åŒ…å«æ–‡æœ¬çš„åˆ—åï¼ˆé»˜è®¤ï¼š'å¾®åšæ­£æ–‡'ï¼‰/ Column containing text
- `TOKENIZE_KEYWORD_COLUMN` - åŒ…å«å…³é”®è¯çš„åˆ—åï¼ˆé»˜è®¤ï¼š'å…³é”®è¯'ï¼‰/ Column containing keywords
- `TOKENIZE_WORD_LENGTH_RANGE` - è¯é•¿èŒƒå›´ï¼ˆé»˜è®¤ï¼š(2, 10)ï¼‰/ Word length range (min, max)

**è¯é¢‘ç»Ÿè®¡å‚æ•° | *Word Frequency Parameters***
- `WORD_FREQ_TOP_N` - ä¿ç•™å‰ N ä¸ªè¯ï¼ˆé»˜è®¤ï¼š50ï¼‰/ Keep top N words

**è¯äº‘å‚æ•° | *Word Cloud Parameters***
- `WORDCLOUD_KEYWORD_COLUMN` - å…³é”®è¯åˆ—åï¼ˆé»˜è®¤ï¼š'å…³é”®è¯'ï¼‰/ Keyword column name
- `WORDCLOUD_WORD_COLUMN` - è¯åˆ—åï¼ˆé»˜è®¤ï¼š'è¯'ï¼‰/ Word column name
- `WORDCLOUD_FREQ_COLUMN` - è¯é¢‘åˆ—åï¼ˆé»˜è®¤ï¼š'è¯é¢‘'ï¼‰/ Frequency column name
- `WORDCLOUD_TOP_N` - æ¯ä¸ªå…³é”®è¯ä¿ç•™çš„è¯æ•°ï¼ˆé»˜è®¤ï¼š30ï¼‰/ Top N words per keyword
- `WORDCLOUD_FONT_PATH` - å­—ä½“æ–‡ä»¶è·¯å¾„ / Font path (for Chinese characters)
- `WORDCLOUD_COLORS_LIST` - é¢œè‰²åˆ—è¡¨ / Color list
- `WORDCLOUD_COLS` - è¯äº‘æ˜¾ç¤ºåˆ—æ•°ï¼ˆé»˜è®¤ï¼š3ï¼‰/ Number of columns
- `WORDCLOUD_FIGSIZE` - å›¾è¡¨å°ºå¯¸ï¼ˆé»˜è®¤ï¼š(15, 12)ï¼‰/ Figure size (width, height)
- `WORDCLOUD_PREFER_HORIZONTAL` - ä¼˜å…ˆæ°´å¹³å¸ƒç½®è¯ï¼ˆé»˜è®¤ï¼š0.7ï¼‰/ Prefer horizontal layout (0-1)
- `WORDCLOUD_RELATIVE_SCALING` - è¯å¤§å°ç›¸å¯¹ç¼©æ”¾ï¼ˆé»˜è®¤ï¼š0.5ï¼‰/ Relative scaling (0-1)
- `WORDCLOUD_MIN_FONT_SIZE` - æœ€å°å­—ä½“å¤§å°ï¼ˆé»˜è®¤ï¼š10ï¼‰/ Minimum font size
- `WORDCLOUD_SHOW` - æ˜¯å¦æ˜¾ç¤ºè¯äº‘ï¼ˆé»˜è®¤ï¼šTrueï¼‰/ Display word clouds

---

### 2. AI åˆ†ææ¨¡å— | *AI Analysis Module (`analyzer`)*

#### æ ¸å¿ƒè„šæœ¬ | *Core Scripts*

**`summary.py`** - ä¸¤é˜¶æ®µ AI åˆ†æè„šæœ¬ / Two-stage AI analysis script

è¿è¡Œæ–¹å¼ | Usage:
```bash
python analyzer/summary.py
```

**åˆ†ææµç¨‹**ï¼š
1. **é˜¶æ®µä¸€** - å…³é”®è¯å¾®è§‚åˆ†æ / Stage 1 - Keyword fact extraction
   - é€ä¸ªå…³é”®è¯åˆ†æï¼Œæå–æ ¸å¿ƒäº‹å®å’Œè§‚ç‚¹
   - *Per-keyword analysis to extract facts and key points*
   - è¾“å‡ºï¼š`stage1_keyword_analysis.csv` / Output: CSV file with per-keyword analysis

2. **é˜¶æ®µäºŒ** - å®è§‚å…³è”åˆ†æ / Stage 2 - Correlation analysis
   - åˆ†æå…³é”®è¯ä¹‹é—´çš„å…³è”å’Œé€»è¾‘å…³ç³»
   - *Analyze correlations and logical relationships between keywords*
   - è¾“å‡ºï¼š`stage2_correlation_analysis_report.md` / Output: Markdown report

3. **çŸ¥è¯†åº“ç”Ÿæˆ** - ç»¼åˆèƒŒæ™¯çŸ¥è¯†åº“ / Knowledge base - Synthesized background knowledge
   - è¾“å‡ºï¼š`final_context_knowledge_base.txt` / Output: Knowledge base text file

**`labeling_testing.py`** - æ‰“æ ‡æµ‹è¯•å·¥å…· / Labeling testing tool

åŠŸèƒ½ï¼šæµ‹è¯•æ‰“æ ‡æ•ˆæœï¼Œè‡ªåŠ¨æ›´æ–° prompt ä¸­çš„èƒŒæ™¯çŸ¥è¯†

*Features: Test labeling effectiveness, auto-update background knowledge in prompts*

```bash
# 1. æ›´æ–°èƒŒæ™¯çŸ¥è¯† / Update background knowledge in prompt
python analyzer/labeling_testing.py update

# 2. æµ‹è¯•æ‰“æ ‡ / Test labeling effectiveness
python analyzer/labeling_testing.py test --n 20

# 3. å®Œæ•´æµç¨‹ï¼šæ›´æ–° + æµ‹è¯• / Complete workflow: update + test
python analyzer/labeling_testing.py update && python analyzer/labeling_testing.py test
```

é…ç½®å‚æ•°å‚è€ƒ `analyzer/settings.py` ä¸­çš„ï¼š
- `TEST_POST_LIST` - ç”¨äºæ‰“æ ‡çš„å¾®åšåˆ—è¡¨ CSV æ–‡ä»¶
- `TEST_SAMPLE_SIZE` - æµ‹è¯•æ ·æœ¬æ•°ï¼ˆé»˜è®¤ï¼š20ï¼‰
- `API_KEY`ã€`MODEL_NAME` ç­‰ API é…ç½®

*Configuration parameters in `analyzer/settings.py`:*
- *`TEST_POST_LIST` - CSV file with posts for labeling*
- *`TEST_SAMPLE_SIZE` - Number of samples to test (default: 20)*
- *`API_KEY`, `MODEL_NAME` and other API settings*

**`batch_generator.py`** - æ‰¹é‡æ¨ç†æ–‡ä»¶ç”Ÿæˆå™¨ / Batch inference file generator

ç”¨é€”ï¼šä¸ºé˜¿é‡Œäº‘ç™¾ç‚¼ API çš„æ‰¹é‡æ¨ç†åŠŸèƒ½ç”Ÿæˆ JSONL æ ¼å¼çš„è¯·æ±‚æ–‡ä»¶

*Purpose: Generate JSONL request files for Alibaba Cloud Bailian batch inference API*

```bash
# ç”Ÿæˆæ‰¹é‡æ¨ç†æ–‡ä»¶ / Generate batch inference file
python analyzer/batch_generator.py

# æˆ–æŒ‡å®šè¾“å…¥è¾“å‡ºæ–‡ä»¶ / Or specify input/output files
python analyzer/batch_generator.py \
  --input analyzer/data/post_list.csv \
  --output analyzer/data/batch_generated.jsonl \
  --prompt analyzer/prompts/labeling_prompt.txt \
  --model qwen-plus
```

è¾“å‡ºæ–‡ä»¶ä¸º JSONL æ ¼å¼ï¼Œæ¯è¡Œä¸€ä¸ª JSON è¯·æ±‚å¯¹è±¡ï¼Œå¯ç›´æ¥ä¸Šä¼ è‡³é˜¿é‡Œäº‘ç™¾ç‚¼æ‰¹é‡æ¨ç† APIã€‚

*Output file in JSONL format (one JSON request per line), ready for Alibaba Cloud Bailian batch API.*

é…ç½®å‚æ•°å‚è€ƒ `analyzer/settings.py` ä¸­çš„ï¼š
- `TEST_POST_LIST` - è¾“å…¥çš„å¾®åšåˆ—è¡¨ CSV æ–‡ä»¶
- `MODEL_NAME` - ä½¿ç”¨çš„æ¨¡å‹
- å…¶ä»– API é…ç½®

*Configuration parameters in `analyzer/settings.py`:*
- *`TEST_POST_LIST` - Input CSV file with posts*
- *`MODEL_NAME` - Model to use*
- *Other API settings*

#### å‚æ•°é…ç½® | *Settings Configuration (`analyzer/settings.py`)*

**API é…ç½® | *API Configuration***
- `API_KEY` - é˜¿é‡Œäº‘ç™¾ç‚¼ API Keyï¼ˆå¿…é¡»ä¿®æ”¹ï¼‰/ Alibaba Cloud Bailian API Key (MUST MODIFY)
- `BASE_URL` - API æ¥å…¥ç‚¹ï¼ˆé»˜è®¤ï¼š`https://dashscope.aliyuncs.com/compatible-mode/v1`ï¼‰/ API endpoint
- `MODEL_NAME` - ä½¿ç”¨çš„æ¨¡å‹ï¼ˆé»˜è®¤ï¼š'qwen-plus'ï¼‰/ Model to use (default: qwen-plus)

**åˆ†æå‚æ•° | *Analysis Parameters***
- `TEMP_STAGE_1` - é˜¶æ®µä¸€æ¸©åº¦å‚æ•°ï¼ˆé»˜è®¤ï¼š0.3ï¼‰/ Stage 1 temperature for deterministic results
- `TEMP_STAGE_2` - é˜¶æ®µäºŒæ¸©åº¦å‚æ•°ï¼ˆé»˜è®¤ï¼š0.5ï¼‰/ Stage 2 temperature for balanced results
- `MAX_TEXT_LENGTH` - å•æ¬¡è¯·æ±‚æœ€å¤§æ–‡æœ¬é•¿åº¦ï¼ˆé»˜è®¤ï¼š25000ï¼‰/ Max text per request (to control cost)
- `MAX_RETRIES` - API è°ƒç”¨å¤±è´¥é‡è¯•æ¬¡æ•°ï¼ˆé»˜è®¤ï¼š3ï¼‰/ Retry count on API failure

**æ–‡ä»¶è·¯å¾„ | *File Paths***
- `INPUT_FILE` - å¾…åˆ†æçš„æ•°æ®æ–‡ä»¶ï¼ˆé»˜è®¤ï¼š'analyzer/data/context_posts.csv'ï¼‰/ CSV file to analyze
- `PROMPT_DIR` - Prompt æ–‡ä»¶å¤¹ï¼ˆé»˜è®¤ï¼š'analyzer/prompts'ï¼‰/ Prompt templates folder
  - `PROMPT_SYSTEM_FILE` - ç³»ç»Ÿ prompt / System prompt file
  - `PROMPT_STAGE1_FILE` - é˜¶æ®µä¸€ prompt / Stage 1 prompt file
  - `PROMPT_STAGE2_FILE` - é˜¶æ®µäºŒ prompt / Stage 2 prompt file
- `OUTPUT_STAGE1_CSV` - é˜¶æ®µä¸€è¾“å‡ºï¼ˆé»˜è®¤ï¼š'analyzer/data/stage1_keyword_analysis.csv'ï¼‰
- `OUTPUT_STAGE2_MD` - é˜¶æ®µäºŒè¾“å‡ºï¼ˆé»˜è®¤ï¼š'analyzer/data/stage2_correlation_analysis_report.md'ï¼‰
- `OUTPUT_FINAL_CONTEXT` - çŸ¥è¯†åº“è¾“å‡ºï¼ˆé»˜è®¤ï¼š'analyzer/data/final_context_knowledge_base.txt'ï¼‰

**æ‰“æ ‡æµ‹è¯•å‚æ•° | *Labeling Test Parameters***
- `TEST_POST_LIST` - ç”¨äºæ‰“æ ‡çš„å¾®åšåˆ—è¡¨ï¼ˆé»˜è®¤ï¼š'analyzer/data/post_list.csv'ï¼‰/ Post list for testing
- `TEST_SAMPLE_SIZE` - æµ‹è¯•æ ·æœ¬æ•°ï¼ˆé»˜è®¤ï¼š20ï¼‰/ Number of samples to test

---

## ä¾èµ–é¡¹ | *Dependencies*

### ä¸»è¦åº“ | *Main Libraries*

- **Python 3.8+**
- **pandas** - æ•°æ®å¤„ç† / Data processing
- **jieba** - ä¸­æ–‡åˆ†è¯ / Chinese word segmentation
- **wordcloud** - è¯äº‘ç”Ÿæˆ / Word cloud generation
- **matplotlib** - æ•°æ®å¯è§†åŒ– / Data visualization
- **openai** - OpenAI API å®¢æˆ·ç«¯ï¼ˆç”¨äºé˜¿é‡Œäº‘å…¼å®¹æ¥å£ï¼‰/ OpenAI client for Alibaba Cloud compatible API
- **requests** - HTTP è¯·æ±‚ / HTTP requests

### Submodule ä¾èµ– | *Submodule Dependencies*

- **[weibo-posts-processing](https://github.com/mikrokozmoz/weibo-posts-processing)** - å¾®åšæ•°æ®å¤„ç†åº“ / Weibo data processing library
  - æä¾›æ•°æ®é¢„å¤„ç†ã€å»é‡ã€åˆ†è¯ã€è¯äº‘ç­‰æ ¸å¿ƒåŠŸèƒ½
  - Provides core functionality for preprocessing, deduplication, tokenization, word clouds

---

## å¸¸è§é—®é¢˜ | *FAQ*

### Q: å¦‚ä½•è·å–é˜¿é‡Œäº‘ç™¾ç‚¼ API Keyï¼Ÿ

1. è®¿é—® https://bailian.console.aliyun.com/
2. æ³¨å†Œæˆ–ç™»å½•é˜¿é‡Œäº‘è´¦å·
3. åœ¨æ§åˆ¶å°åˆ›å»º API Key
4. å¤åˆ¶ API Key åˆ° `analyzer/settings.py` çš„ `API_KEY` å­—æ®µ

### *Q: How do I get Alibaba Cloud Bailian API Key?*

1. *Visit https://bailian.console.aliyun.com/*
2. *Register or login to Alibaba Cloud account*
3. *Create API Key in the console*
4. *Copy the API Key to `API_KEY` field in `analyzer/settings.py`*

---

## è®¸å¯è¯ | *License*

MIT License

---

## ç›¸å…³é¡¹ç›® | *Related Projects*

- [weibo-posts-processing](https://github.com/mikrokozmoz/weibo-posts-processing) - å¾®åšæ•°æ®å¤„ç†åº“ / Weibo data processing library
- [weibo-search](https://github.com/dataabc/weibo-search) - å¾®åšçˆ¬è™«æ¡†æ¶ / Weibo crawler framework

---

## è´¡çŒ® | *Contributing*

æ¬¢è¿æäº¤ issue æˆ– pull requestï¼

Contributions via issues or pull requests are welcome!
