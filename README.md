# å¾®åšçƒ­æœåˆ†æå·¥å…· | Weibo Hot Search Analyzer

ä¸€ä¸ªç”¨äºå¾®åšæ•°æ®åˆ†æå’Œå¤„ç†çš„å®Œæ•´å·¥å…·é“¾ã€‚æ”¯æŒæ™ºèƒ½å»é‡ã€åˆ†è¯ç»Ÿè®¡ã€è¯äº‘ç”Ÿæˆã€ä»¥åŠAIé©±åŠ¨çš„æ‘˜è¦åˆ†æã€‚ï¼ˆå¯é€‰ï¼šæ”¯æŒæ•°æ®åˆå¹¶åŠŸèƒ½ï¼‰

A complete toolkit for Weibo data analysis and processing. Supports smart deduplication, word frequency statistics, word cloud generation, and AI-driven summary analysis. (Optional: supports data merging functionality)

---

## é¡¹ç›®ç®€ä»‹ | Project Overview

æœ¬é¡¹ç›®ä¸“æ³¨äºå¾®åšæ•°æ®çš„åˆ†æå’Œå¤„ç†ï¼Œé€šè¿‡æ¨¡å—åŒ–çš„è®¾è®¡ï¼Œç”¨æˆ·å¯ä»¥çµæ´»åœ°ï¼š
- ä½¿ç”¨ `utils` æ¨¡å—è¿›è¡Œæ•°æ®é¢„å¤„ç†ï¼ˆå»é‡ã€åˆ†è¯ã€è¯äº‘ç”Ÿæˆç­‰ï¼‰
- ä½¿ç”¨ `analyzer` æ¨¡å—è¿›è¡Œ AI é©±åŠ¨çš„åˆ†æå’Œæ‘˜è¦ç”Ÿæˆ
- è‡ªå®šä¹‰å‚æ•°ï¼Œä¸€é”®æ‰§è¡Œå®Œæ•´çš„æ•°æ®å¤„ç†æµç¨‹
- ï¼ˆå¯é€‰ï¼‰ä½¿ç”¨æ•°æ®åˆå¹¶åŠŸèƒ½æ•´åˆå¤šä¸ªæ¥æºçš„æ•°æ®

*This project focuses on analysis and processing of Weibo data. With a modular design, users can flexibly:*
- *Use the `utils` module for data preprocessing (deduplication, tokenization, word cloud generation, etc.)*
- *Use the `analyzer` module for AI-driven analysis and summary generation*
- *Customize parameters and execute the complete data processing pipeline with a single command*
- *(Optional) Use data merging functionality to consolidate data from multiple sources*

---

## åŠŸèƒ½ç‰¹æ€§ | Features

### æ•°æ®å¤„ç†æ¨¡å— | Data Processing Module

- **çµæ´»çš„æ•°æ®åŠ è½½** ğŸ“‚ï¼šä»æ–‡ä»¶å¤¹ç›´æ¥åŠ è½½ CSV æ–‡ä»¶
- **æ™ºèƒ½å»é‡** âœ¨ï¼šæ”¯æŒè‡ªå®šä¹‰ç›¸ä¼¼åº¦é˜ˆå€¼çš„å»é‡ï¼Œä¿ç•™æœ€æ—©å‘å¸ƒçš„è®°å½•
- **è¯é¢˜æå–** ğŸ·ï¸ï¼šè‡ªåŠ¨ä»å¾®åšä¸­æå–å‰ä¸‰ä¸ªè¯é¢˜
- **åˆ†è¯ç»Ÿè®¡** ğŸ”¤ï¼šåŸºäº jieba çš„ä¸­æ–‡åˆ†è¯å’Œè¯é¢‘ç»Ÿè®¡
- **è¯äº‘ç”Ÿæˆ** â˜ï¸ï¼šæŒ‰å…³é”®è¯è‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡è¯äº‘å›¾
- **å‚æ•°é›†ä¸­ç®¡ç†** âš™ï¸ï¼šæ‰€æœ‰å¤„ç†å‚æ•°åœ¨ `utils/settings.py` ä¸­é…ç½®

<br>

- *Flexible data loading from CSV files in a folder* 
- *Smart deduplication with custom similarity threshold* 
- *Automatic topic extraction from posts* 
- *Chinese word segmentation and frequency statistics based on jieba*
- *Auto-generate high-quality word clouds per keyword*
- *Centralized parameter management in `utils/settings.py`*

### AI åˆ†ææ¨¡å— | AI Analysis Module

- **ä¸¤é˜¶æ®µåˆ†æ** ğŸ§ ï¼šç¬¬ä¸€é˜¶æ®µå¾®è§‚äº‹å®æå–ï¼ˆé€å…³é”®è¯åˆ†æï¼‰+ ç¬¬äºŒé˜¶æ®µå®è§‚å…³è”åˆ†æï¼ˆå…¨å±€äº‹ä»¶å…³è”ï¼‰
- **çŸ¥è¯†åº“ç”Ÿæˆ** ğŸ“šï¼šè‡ªåŠ¨ç”ŸæˆèƒŒæ™¯çŸ¥è¯†åº“ï¼Œæ”¯æŒä¸‹æ¸¸åº”ç”¨
- **æ¨¡å‹çµæ´»é…ç½®** ğŸ”§ï¼šæ”¯æŒå¤šç§é˜¿é‡Œäº‘ç™¾ç‚¼å¤§æ¨¡å‹ï¼ˆqwen-plusã€qwen-max ç­‰ï¼‰
- **å®Œå–„çš„é”™è¯¯å¤„ç†** ğŸ›¡ï¸ï¼šAPI è°ƒç”¨å¤±è´¥è‡ªåŠ¨é‡è¯•æœºåˆ¶

<br>

- *Two-stage analysis (micro-fact extraction + macro-correlation analysis)*
- *Automatic knowledge base generation for downstream applications*
- *Flexible model configuration supporting multiple Alibaba Cloud models*
- *Robust error handling with automatic retry on API failures*

---

## å¿«é€Ÿä¸Šæ‰‹ | Quick Start

### 1. ç¯å¢ƒå®‰è£… | Installation

```bash
# å…‹éš†é¡¹ç›®
# Clone the repository
git clone --recursive https://github.com/mikrokozmoz/weibo-hot-analyzer
cd weibo-hot-analyzer

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼ˆå¯é€‰ä½†æ¨èï¼‰
# Create virtual environment (optional but recommended)
python -m venv venv
source venv/bin/activate  # on Windows: venv\Scripts\activate

# å®‰è£…ä¾èµ–
# Install dependencies
pip install -r requirements.txt
```

### 2. æ•°æ®é¢„å¤„ç† | Data Preprocessing

ä½¿ç”¨ `utils` æ¨¡å—å¤„ç† CSV æ•°æ®ï¼ˆå»é‡ã€åˆ†è¯ã€è¯äº‘ç­‰ï¼‰ã€‚

Use the `utils` module to process CSV data (deduplication, tokenization, word clouds, etc.).

#### 2.1 é…ç½®å¤„ç†å‚æ•° | Configure Processing Parameters

ç¼–è¾‘ `utils/settings.py`ï¼Œè®¾ç½®ï¼š
- `LOAD_POSTS_FOLDER_PATH`ï¼šåŒ…å« CSV æ–‡ä»¶çš„æ–‡ä»¶å¤¹è·¯å¾„
- å…¶ä»–å¤„ç†å‚æ•°ï¼ˆå»é‡é˜ˆå€¼ã€è¯é•¿åº¦èŒƒå›´ç­‰ï¼‰

Edit `utils/settings.py` to set:
- `LOAD_POSTS_FOLDER_PATH`: Path to folder containing CSV files
- Other processing parameters (deduplication threshold, word length range, etc.)

#### 2.2 æ‰§è¡Œæ•°æ®å¤„ç† | Execute Data Processing

```bash
# æ–¹å¼ä¸€ï¼šä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°
# Method 1: Using command-line arguments
python -m utils --load_files_from_folder --dedupe

# æ–¹å¼äºŒï¼šåœ¨ Python ä»£ç ä¸­è°ƒç”¨
# Method 2: Call in Python code
from utils import data_processing

result = data_processing(
    load_files_from_folder=True,
    dedupe=True
)
```

**æ”¯æŒçš„å‚æ•°** | *Supported parameters*:
- `--load_files_from_folder`ï¼šä»æ–‡ä»¶å¤¹åŠ è½½ CSV / Load CSV from folder
- `--extract_topics`ï¼šæå–è¯é¢˜ / Extract topics
- `--dedupe`ï¼šå»é‡å¤„ç† / Deduplication
- `--tokenize`ï¼šåˆ†è¯ç»Ÿè®¡ / Word segmentation & frequency
- `--word_frequency`ï¼šç”Ÿæˆè¯é¢‘è¡¨ / Generate frequency table
- `--create_wordcloud`ï¼šç”Ÿæˆè¯äº‘ / Generate word clouds

### 3. AI é©±åŠ¨åˆ†æ | AI-Driven Analysis

ä½¿ç”¨ `analyzer` æ¨¡å—è¿›è¡Œ AI åˆ†æå’Œæ‘˜è¦ç”Ÿæˆã€‚

Use the `analyzer` module for AI analysis and summary generation.

#### 3.1 é…ç½® AI å‚æ•° | Configure AI Parameters

ç¼–è¾‘ `analyzer/settings.py`ï¼Œè®¾ç½®ï¼š
- `API_KEY`ï¼šé˜¿é‡Œäº‘ç™¾ç‚¼ API Keyï¼ˆä» https://bailian.console.aliyun.com/ è·å–ï¼‰
- `MODEL_NAME`ï¼šé€‰æ‹©æ¨¡å‹ï¼ˆæ¨è qwen-plus æˆ– qwen-maxï¼‰
- `INPUT_FILE`ï¼šå¾…åˆ†æçš„ CSV æ–‡ä»¶è·¯å¾„
- å…¶ä»–åˆ†æå‚æ•°

Edit `analyzer/settings.py` to set:
- `API_KEY`: Alibaba Cloud Bailian API Key (get from https://bailian.console.aliyun.com/)
- `MODEL_NAME`: Choose model (recommended qwen-plus or qwen-max)
- `INPUT_FILE`: Path to CSV file to analyze
- Other analysis parameters

#### 3.2 è¿è¡Œåˆ†æ | Run Analysis

```bash
# æ‰§è¡Œä¸¤é˜¶æ®µ AI åˆ†æ
# Run two-stage AI analysis
python -m analyzer

# è¾“å‡ºç»“æœï¼š
# Output results:
# - stage1_keyword_analysis.csv: å…³é”®è¯å¾®è§‚åˆ†æ / Micro-analysis per keyword
# - stage2_correlation_analysis_report.md: å®è§‚å…³è”æŠ¥å‘Š / Keyword-correlation report
# - final_context_knowledge_base.txt: å®Œæ•´çŸ¥è¯†åº“ / Complete knowledge base
```

---

## é¡¹ç›®ç»“æ„ | Project Structure

```
weibo-hot-analyzer/
â”œâ”€â”€ analyzer/                          # AI é©±åŠ¨åˆ†ææ¨¡å— / AI Analysis Module
â”‚   â”œâ”€â”€ settings.py                   # AI åˆ†æå‚æ•°é…ç½® / AI parameters
â”‚   â”œâ”€â”€ summary.py                    # ä¸¤é˜¶æ®µåˆ†æè„šæœ¬ / Two-stage analysis script
â”‚   â”œâ”€â”€ prompts/                      # Prompt æ¨¡æ¿æ–‡ä»¶ / Prompt templates
â”‚   â”‚   â”œâ”€â”€ sys_prompt                # ç³»ç»Ÿ prompt
â”‚   â”‚   â”œâ”€â”€ keyword_prompt            # å…³é”®è¯åˆ†æ prompt
â”‚   â”‚   â””â”€â”€ correlation_prompt        # å…³è”åˆ†æ prompt
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ utils/                             # æ•°æ®å¤„ç†æ¨¡å— / Data Processing Module
â”‚   â”œâ”€â”€ settings.py                   # å¤„ç†å‚æ•°é…ç½® / Processing parameters
â”‚   â”œâ”€â”€ data_processing.py            # å…¥å£å‡½æ•° / Entry function
â”‚   â”œâ”€â”€ __main__.py                   # å‘½ä»¤è¡Œæ¥å£ / CLI interface
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ processing/                        # Submodule: æ•°æ®å¤„ç†åº“
â”‚   â””â”€â”€ post_analysis/
â”‚       â”œâ”€â”€ pre_processing.py         # æ•°æ®åŠ è½½ã€å»é‡ã€è¯é¢˜æå–
â”‚       â”œâ”€â”€ corpus_analysis.py        # åˆ†è¯ã€è¯é¢‘ã€è¯äº‘
â”‚       â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ files/                             # æ•°æ®æ–‡ä»¶ç›®å½• / Data files directory
â”œâ”€â”€ requirements.txt                   # é¡¹ç›®ä¾èµ– / Dependencies
â”œâ”€â”€ README.md                          # é¡¹ç›®è¯´æ˜ï¼ˆæœ¬æ–‡ä»¶ï¼‰
â””â”€â”€ .gitmodules                        # Submodule é…ç½®
```

---

## æ ¸å¿ƒæ¦‚å¿µ | Core Concepts

### å»é‡ç›¸ä¼¼åº¦é˜ˆå€¼ | Deduplication Similarity Threshold

å»é‡è¿‡ç¨‹ä¸­ï¼Œç›¸ä¼¼åº¦é˜ˆå€¼æ§åˆ¶å»é‡çš„ä¸¥æ ¼ç¨‹åº¦ï¼š

The similarity threshold controls the strictness of deduplication:

| é˜ˆå€¼èŒƒå›´ | è¯´æ˜ | å»ºè®®åœºæ™¯ |
|---------|------|--------|
| 0.70-0.80 | å®½æ¾å»é‡ï¼Œç§»é™¤æ›´å¤šé‡å¤ | éœ€è¦é«˜è´¨é‡å”¯ä¸€å†…å®¹çš„åˆ†æ |
| 0.80-0.90 | å¹³è¡¡å»é‡ï¼Œæ¨èä½¿ç”¨ | ä¸€èˆ¬æ•°æ®åˆ†æ |
| 0.90-0.99 | ä¸¥æ ¼å»é‡ï¼Œç§»é™¤è¾ƒå°‘é‡å¤ | ä¿ç•™ç»†å¾®å·®å¼‚å†…å®¹ |

| Threshold | Description | Recommended Scenario |
|---------|---------|--------|
| 0.70-0.80 | Loose dedup, remove more | High-quality unique content analysis |
| 0.80-0.90 | Balanced dedup (recommended) | General data analysis |
| 0.90-0.99 | Strict dedup, remove fewer | Preserve subtle differences |

### AI æ¨¡å‹é€‰æ‹© | AI Model Selection

æ”¯æŒçš„é˜¿é‡Œäº‘ç™¾ç‚¼æ¨¡å‹ï¼š

Supported Alibaba Cloud Bailian models:

- **qwen-plus**ï¼šæ€§ä»·æ¯”é«˜ï¼Œé€‚åˆå¤§éƒ¨åˆ†ä»»åŠ¡ / Cost-effective, suitable for most tasks
- **qwen-max**ï¼šé€»è¾‘èƒ½åŠ›å¼ºï¼Œé€‚åˆå¤æ‚åˆ†æ / Stronger logic, suitable for complex analysis
- **qwen-turbo**ï¼šé€Ÿåº¦å¿«ï¼Œé€‚åˆå¿«é€Ÿå¤„ç† / Fast speed, suitable for quick processing

---

## ä¾èµ–é¡¹ | Dependencies

### ä¸»è¦åº“ | Main Libraries

- **Python 3.8+**
- **pandas** - æ•°æ®å¤„ç† / Data processing
- **jieba** - ä¸­æ–‡åˆ†è¯ / Chinese word segmentation
- **wordcloud** - è¯äº‘ç”Ÿæˆ / Word cloud generation
- **matplotlib** - æ•°æ®å¯è§†åŒ– / Data visualization
- **openai** - OpenAI API å®¢æˆ·ç«¯ï¼ˆç”¨äºé˜¿é‡Œäº‘å…¼å®¹æ¥å£ï¼‰/ OpenAI client for Alibaba Cloud compatible API
- **requests** - HTTP è¯·æ±‚ / HTTP requests

### Submodule ä¾èµ– | Submodule Dependencies

- **[weibo-posts-processing](https://github.com/mikrokozmoz/weibo-posts-processing)** - å¾®åšæ•°æ®å¤„ç†åº“ / Weibo data processing library
  - æä¾›æ•°æ®é¢„å¤„ç†ã€å»é‡ã€åˆ†è¯ã€è¯äº‘ç­‰æ ¸å¿ƒåŠŸèƒ½
  - Provides core functionality for preprocessing, deduplication, tokenization, word clouds

---

## å¸¸è§é—®é¢˜ | FAQ

### Q: å¦‚ä½•è·å–é˜¿é‡Œäº‘ç™¾ç‚¼ API Keyï¼Ÿ

A: 
1. è®¿é—® https://bailian.console.aliyun.com/
2. æ³¨å†Œæˆ–ç™»å½•é˜¿é‡Œäº‘è´¦å·
3. åœ¨æ§åˆ¶å°åˆ›å»º API Key
4. å¤åˆ¶ API Key åˆ° `analyzer/settings.py` çš„ `API_KEY` å­—æ®µ

### Q: How do I get Alibaba Cloud Bailian API Key?

A:
1. Visit https://bailian.console.aliyun.com/
2. Register or login to Alibaba Cloud account
3. Create API Key in the console
4. Copy the API Key to `API_KEY` field in `analyzer/settings.py`

---

## è®¸å¯è¯ | License

MIT License

---

## ç›¸å…³é¡¹ç›® | Related Projects

- [weibo-posts-processing](https://github.com/mikrokozmoz/weibo-posts-processing) - å¾®åšæ•°æ®å¤„ç†åº“ / Weibo data processing library
- [weibo-search](https://github.com/dataabc/weibo-search) - å¾®åšçˆ¬è™«æ¡†æ¶ / Weibo crawler framework

---

## è´¡çŒ® | Contributing

æ¬¢è¿æäº¤ issue æˆ– pull requestï¼

Contributions via issues or pull requests are welcome!
